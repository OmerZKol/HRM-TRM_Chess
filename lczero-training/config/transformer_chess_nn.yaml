# Configuration for Transformer Chess Neural Network
# Model: TransformerChessNet - Transformer-based architecture with attention policy

# Training parameters
batch_size: 256
learning_rate: 0.0001
epochs: 30

name: "transformer"

# Data sampling
sample_rate: 1  # Use all data (set to higher value for faster training with less data)

# Training optimizations
num_workers: 4  # Number of parallel data loading workers (0 = single-threaded)
use_amp: true  # Use Automatic Mixed Precision training (faster on modern GPUs)
gradient_accumulation_steps: 1  # Accumulate gradients over N steps (effective_batch = batch_size * N)

# Loss weights
policy_loss_weight: 1.0
value_loss_weight: 1.0
moves_left_loss_weight: 0.01

# Model architecture
model_type: transformer
input_channels: 112
hidden_size: 512
num_layers: 8
num_heads: 8
expansion: 4.0
policy_size: 1858
max_position_embeddings: 64
rope_base: 10000
use_wdl: true