# Configuration for TRM Chess Model with halt_max_steps=10 WITHOUT q-continue loss
# Model: ChessTRMNet (TinyRecursiveReasoningModel_ACTV1) - Multi-step adaptive reasoning
# This config disables the q-continue loss, using only q-halt loss

# Training parameters
batch_size: 128
learning_rate: 0.0001
epochs: 70

name: "trm_halt10_no_qcontinue"

# Data sampling
sample_rate: 1  # Use all data (set to higher value for faster training with less data)

# Loss weights
policy_loss_weight: 1.0
value_loss_weight: 1.0
moves_left_loss_weight: 0.01

# Model architecture
model_type: trm
board_x: 8
board_y: 8
action_size: 1858

# TRM specific parameters
trm_config:
  seq_len: 64  # Input sequence length (64 chess squares)

  # Reasoning cycles
  H_cycles: 2  # High-level reasoning cycles
  L_cycles: 2  # Low-level reasoning cycles per H-cycle
  H_layers: 1  # Ignored in TRM (uses L_level for both H and L)
  L_layers: 6  # Number of transformer layers in L-level

  # Model dimensions
  hidden_size: 512
  expansion: 2.0
  num_heads: 8

  # Positional encodings
  pos_encodings: rope
  rope_theta: 10000.0

  # Halting configuration - MULTI-STEP with adaptive computation
  halt_max_steps: 10
  halt_exploration_prob: 0.1
  no_ACT_continue: False  # Simplified halting (TRM-specific)

  # Q-loss configuration - DISABLE q-continue loss
  use_q_continue: false

  # Chess-specific configuration (always enabled in TRM)
  square_feature_dim: 112
  board_x: 8
  board_y: 8

  # Output heads
  use_move_prediction: true
  use_value_prediction: true
  use_moves_left_prediction: true
  move_prediction_from_token: 0
  value_prediction_from_token: 0
  moves_left_from_token: 0

  # Positional encoding style
  arc_encoding: true  # tfprocess.py style (concat pos_enc with features)
  pos_enc_dim: 16

  # Head architecture
  use_attention_policy: true  # Attention-based policy head
  use_tensorflow_style_heads: true  # TF-style value/moves heads
  value_embedding_size: 32
  moves_embedding_size: 8

  # TRM-specific options
  mlp_t: false  # Use attention instead of MLP on sequence dimension

  # Data type
  forward_dtype: bfloat16
